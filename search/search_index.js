var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Build an AI Assistant for FSI using Cortex and Document AI","text":""},{"location":"index.html#build-an-ai-assistant-for-fsi-using-cortex-and-document-ai","title":"Build an AI Assistant for FSI using Cortex and Document AI","text":"<p>Hands On Lab</p> <p></p> <p>Introduction</p> <p>Effective stock selection relies on having up-to-date data from diverse sources\u2014research analyst reports, 10-K filings, investor call recordings, historical tick data, and real-time market feeds. To manage this efficiently, a unified platform is essential\u2014one capable of storing and processing all data types, whether PDFs of analyst reports, MP3 investor recordings, or SEC documents. This data must be readily accessible for analysis using the latest large language models (LLMs) such as Anthropic, Gemma, LLaMA, or DeepSeek. Ensuring the trustworthiness and security of generated summaries and insights is critical, especially when they inform high-stakes financial decisions. In addition, developing Agentic AI capabilities allows for natural language question-answering tailored to investors and traders who need precise, real-time insights before executing trades. In this hands-on lab, you\u2019ll learn how to build a Stock Selection Agent from the ground up using the Snowflake AI Data Cloud.</p> <p>Learn how you can leverage the latest AI technologies right within the Snowflake platform. When AI is deeply embedded in your trusted data platform, the possibilities are endless. We will be exploring the processing of both Unstructured and Structured data which will then allow the application of a Cortex Agent to help discover insights by leveraging All Data.</p> <p>Data Sources and Analyst Coverage</p> <p>This hands-on lab utilizes a comprehensive dataset spanning multiple data types and financial analysts to demonstrate real-world AI applications in financial services. You'll work with:</p>"},{"location":"index.html#analyst-research-reports","title":"Analyst Research Reports","text":"<p>The lab features comprehensive analysis from specialized research firms created specifically for this hands-on experience: - Apex Analytics - Growth-focused equity research - Consensus Point - Institutional-grade analysis and recommendations - Momentum Metrics - Quantitative trading insights and momentum analysis - Pinnacle Growth Investors - Growth equity research and sector analysis - Quant-Vestor - Quantitative investment research and algorithmic trading insights - Sterling Partners - Value-oriented investment analysis - Veridian Capital - Comprehensive equity research and market analysis</p>"},{"location":"index.html#multi-modal-data-integration","title":"Multi-Modal Data Integration","text":"<p>Throughout the lab, you'll process and analyze:</p> <ul> <li>Research Reports - Extract structured insights (ratings, price targets, growth forecasts) using Document AI</li> <li>Earnings Call Transcripts - Process audio recordings into searchable text with sentiment analysis</li> <li>Financial Infographics - Extract key metrics from quarterly earnings visuals and charts</li> <li>Real-time Market Data - Live stock price feeds and trading data from Snowflake Marketplace</li> <li>Historical Performance Data - Multi-quarter trends and comparative analysis</li> </ul>"},{"location":"index.html#ai-powered-analysis-journey","title":"AI-Powered Analysis Journey","text":"<p>Your AI assistant will synthesize insights from all these sources to answer questions like: - \"What do Apex Analytics and Consensus Point analysts say about Snowflake's growth prospects?\" - \"How does Momentum Metrics' quantitative analysis compare to Sterling Partners' value assessment?\" - \"What are Veridian Capital's latest price targets and how do they align with earnings call sentiment?\" - \"Based on all available data, should I buy, sell, or hold Snowflake shares?\"</p> <p>This diverse dataset enables you to experience how modern AI can unify structured financial data with unstructured analyst opinions, creating comprehensive investment intelligence.</p> <p> The lab environment</p> <p>A complete lab environment has been built for you automatically. This includes:</p> <ul> <li>Snowflake Account: sfsehol-dry_run_build_ai_assistant_oct_vuifug</li> <li>User: USER</li> <li>Snowflake Virtual Warehouse: DEFAULT_WH</li> <li>Snowflake Database: DATAOPS_EVENT_PROD</li> <li>Schema: DEFAULT_SCHEMA</li> </ul> <p>Structure of the Session</p> <p></p> <p>You will be taken through the AI capabilities of Snowflake with Financial Reporting data. You will be focusing on the Snowflake share price - and will be investigating whether you should BUY, SELL or HOLD</p> <p>Please note that the Analyst reports for this lab are completely fictitious and financial decisions cannot be made based on any of the outcomes of this lab.</p> <p>Order of the Lab</p> <ul> <li> <p>Unstructured Data Processing</p> </li> <li> <p>Structured Data Processing</p> </li> <li> <p>Snowflake Intelligence - Create an AI agent using simple configuration</p> </li> <li> <p>Develop a Streamlit Agent with the Snowflake Agent API - an application to query both Unstructured and Structured Data</p> </li> <li> <p>Navigate through the lab via the side-bar menu</p> </li> </ul> <p>HAVE FUN</p> <ul> <li>Navigate to the lab steps to begin the lab.</li> </ul> <p>This lab environment will disappear!</p> <p>This event is due to end at 2025-11-02 22:00:00+00:00, at which point access will be restricted, and accounts will be removed.</p>"},{"location":"Logging_in.html","title":"Logging in and Cortex Playground","text":""},{"location":"Logging_in.html#logging-in-and-cortex-playground","title":"Logging in and Cortex Playground","text":"<p>Within the previous screen, you should have logged into snowflake which would have opened up in a new tab and should look like this:</p> <p></p> <p>If you cannot find the snowflake URL, you can go back to the registration page and view your unique Snowflake link by clicking here: Personalized Event Homepage</p> <p>Upon logging in, you may be prompted to add your email address.  It is important to input your email address for accessing market place data.  You will be accessing marketplace data in this lab.</p> <p>Once you have logged in, populated your email address and closed down any welcome messages, navigate to the AI &amp; ML section of the navigation bar.</p>"},{"location":"Logging_in.html#snowflake-ai-and-ml-studio","title":"Snowflake AI and ML studio","text":"<p>Snowflake AI and ML studio is a one stop shop  to try out a number of AI functions using a user friendly UI.</p> <p>.  </p> <p>For today's lab, you will be exploring the AI features within Snowflake.  The following features we will be cover as part of the lab:</p> <ul> <li> <p>Cortex Playground</p> <p>The Cortex LLM Playground lets you compare text completions across the multiple large language models available in Cortex AI.</p> </li> <li> <p>Cortex Fine Tuning</p> <p>The Snowflake Cortex Fine-tuning function offers a way to customize large language models for your specific task.</p> </li> <li> <p>Cortex Search</p> <p>Cortex Search enables low-latency, high-quality \u201cfuzzy\u201d search over your Snowflake data.  Cortex Search powers a broad array of search experiences for Snowflake users including Retrieval Augmented Generation (RAG) applications leveraging Large Language Models (LLMs).</p> </li> <li> <p>Cortex Analyst</p> <p>Cortex Analyst is a fully-managed, LLM-powered Snowflake Cortex feature that helps you create applications capable of reliably answering business questions based on your structured data in Snowflake.</p> </li> </ul>"},{"location":"Logging_in.html#cortex-playground","title":"Cortex Playground","text":"<p>Today, we will be covering Cortex Complete using prompt engineering.  This will be covered in various notebooks as part of data processing.  </p> <p>You can try out prompt ideas using the Cortex Playground. </p> <p>Click on Cortex Playground and try asking some questions using a model of choice.</p> <p>Take a look at the following example:</p> <p></p> <p>The answer to the question is making some suggestions of the things I might want to look at before I make this sort of financial decision.  All this information is available in various datasets.</p>"},{"location":"Logging_in.html#document-ai","title":"Document AI","text":"<p>Although Document AI is not an option within the studio page, you will see a link to it within the AI &amp; ML category in the navigation bar.   This is where you will extract features from a variety of unstructured documents and will be the next section of the lab.  Please Proceed to Unstructured Data Processing from the navigation bar.</p>"},{"location":"agents.html","title":"Create Cortex Agents using the Agents API","text":""},{"location":"agents.html#create-cortex-agents-using-the-agents-api","title":"Create Cortex Agents using the Agents API","text":"<p>You have now experienced out of the box agents using simple configuration using Snowflake Intelligence. Snowflake also supports the creation of custom built agents using the Agents API. Let's see how a Cortex Agent using Streamlit will allow users to ask questions about their data all in one place.  </p> <p>If you have completed all the previous steps then this step should just work.</p> <ul> <li>Within Projects&gt;Streamlits click on the streamlit 2_CORTEX_AGENT_SOPHISTICATED</li> </ul> <p>It leverages Streamlit extras which has packaged in styles to customize your app. It also allows you to choose the search service and analyst semantic model during the app configuration. The agent has configurable options for the user such as multiple chart types using Plotly</p> <ul> <li> <p>Ask questions about the data that might appear in the earnings calls or analyst reports.</p> </li> <li> <p>Ask questions about the data that might appear in the stock data or the latest infographics.</p> </li> </ul>"},{"location":"agents.html#sample-questions","title":"Sample Questions","text":"<p>These questions should give answers from both unstructured and structured examples using all the datasets we covered in this lab.</p> <pre><code>give me a table of ratings for each analyst?\n\nwhat happened in the last earnings call?\n\nWhat did Morgan Stanley say about Growth?\n\nwhat are the latest SNOW stock prices over time in the last 12 months?\n\nhow many marketplace listings are in the latest report?\n\nTell me about dynamic tables?\n\nWhat did Shridhar say about revenue in the last earnings call?\n\nshall I buy Snowflake shares?\n</code></pre> <p>If you completed the optional exercise in Cortex Analyst, this is the response you will get if you ask what transcript had the lowest sentiment:</p> <p></p> <p>Try asking the following question to get more information</p> <p>Can you give me more information about that particular earnings call</p> <p>So you should see how convenient it is to bring in both the processing of unstructured datasets and structured datasets to get one holistic view of the data subject (in this case the analysis of Snowflake) in question.</p>"},{"location":"agents.html#editing-the-application","title":"Editing the Application","text":"<p>Your role in this setup allows you to edit the application. This particular agent is quite sophisticated in terms of functionality. So you can understand the key principles of how the agent works, let's switch over to a simpler agent.</p> <p>Within Projects &gt; Streamlit navigate to CORTEX_AGENT_SIMPLE</p> <ul> <li>Press Edit to go into edit mode.</li> </ul> <p>You will notice that the semantic model has been specified as well as the search service - which is what you created earlier.</p> <p>You will notice a series of functions such as:</p> <ul> <li> <p>run_snowflake_query</p> <p>This calls the built-in Agent API - and will treat the question differently depending on the context. For instance, if the answer relates to structured data, it will use the tool spec analyst1 which will attempt to turn the question into a SQL query and will use the YAML file which you created in Cortex Analyst. If however, the answer can only be found from unstructured data, it will use the tools spec cortex_search which will then use the search service to retrieve the information. It will also retrieve up to 10 citations - these will be the chunked text which were extracted earlier on in the lab.</p> </li> <li> <p>process_sse_response</p> <p>This is about parsing the results in something readable as the original response will be a JSON payload</p> </li> <li> <p>execute_cortex_complete_sql</p> <p>This uses the LLM to attempt to create a chart based on what is in the dataset. The type of LLM is defined using the model variable.</p> </li> <li> <p>extract_python_code</p> <p>This is used to return the Python result (which will be a Streamlit chart) into executable code.</p> </li> <li> <p>replace_chart_function</p> <p>This uses the same variables created for the suggested chart but allows for alternative Streamlit charts.</p> </li> <li> <p>def main</p> <p>This is the initial (parent) function which is executed first and calls the other functions when appropriate. </p> </li> </ul> <p>If you would like to make any changes to this application, you will need to duplicate it. This is because the Streamlit app is managed by an external application.</p> <p>If you duplicate the application using the duplicate button, all files associated to the application will be copied with it.</p>"},{"location":"agents.html#conclusion","title":"Conclusion","text":"<p>The Cortex Agent provides a unified way to query both structured and unstructured datasets, enabling users to gain insights from diverse data sources within a single application. By leveraging Streamlit\u2019s customization capabilities and integrating Snowflake\u2019s semantic model and search service, users can seamlessly retrieve information from earnings calls, analyst reports, stock data, and more. The built-in functions ensure efficient querying, data parsing, and visualization, making it easier to analyze financial and market trends.</p>"},{"location":"agents.html#resources","title":"Resources","text":"<ul> <li> <p>Click here to view and download the code used in this lab.</p> </li> <li> <p>Click here to analyze earnings call transcripts with Cortex AI</p> </li> <li> <p>Click here to run a quickstart on Cortex Agents</p> </li> </ul>"},{"location":"analyst.html","title":"Analyse your data with Cortex Analyst","text":""},{"location":"analyst.html#analyse-your-data-with-cortex-analyst","title":"Analyse your data with Cortex Analyst","text":"<p>In this section you will learn how to build a dynamic data explorer using Cortex Analyst. Traditionally, users would digest structured information through reports. Over time, reports turned into dashboards and self service capabilities became more and more in demand. However, in order to self serve, users would normally require extensive report building and data engineering expertise. </p> <p>Cortex Analyst's AI allows views to be generated by simply asking questions about the data</p>"},{"location":"analyst.html#examine-the-structured-marketplace-data","title":"Examine the Structured Marketplace Data","text":"<p>First of all, navigate to the Cortex Analyst notebook by navigating to the projects &gt; Notebooks.</p> <p></p> <ul> <li>Open the notebook and press Start in the top right hand corner to start the compute. </li> </ul> <p>Within the self contained notebook, you will perform the following:</p> <ul> <li>Create a new dataset from the market place</li> <li>Visualise the data in Streamlit</li> </ul> <p>Once you have completed the notebook, return here</p>"},{"location":"analyst.html#use-cortex-analyst-to-explore-the-data","title":"Use Cortex Analyst to Explore the data","text":"<p>It is easy to create an app in order to gain insights from structured data.  Cortex Analyst allows the user to ask questions in natural language and will return the result in an appropiate format.  Let's Begin:</p> <ul> <li>From the navigation bar on the right, hover over the AI &amp; ML icon and click on Studio</li> </ul> <p></p> <ul> <li> <p>When prompted, end the notebook session to proceed.</p> </li> <li> <p>Press Try on Cortex Analyst.</p> </li> </ul> <p></p> <ul> <li> <p>Within the tabs at the top of the screen, switch to Semantic models</p> </li> <li> <p>Under database, select ACCELERATE_AI_IN_FSI then CORTEX_ANALYST for schema</p> </li> <li> <p>select CORTEX_ANALYST for the stage.</p> </li> <li> <p>you will see a model called semantic_model.yaml  has already been created.  Here you can open the model to view how it's constructed.  This model will be used for the agent later on. </p> </li> <li> <p>Click on the semantic_model.yaml to open the model.</p> </li> </ul> <p></p> <p>You will note that as per the screenshot above, the model navigation menu is open.  The Semantic model includes 5 logical tables.  These tables have all been created earlier in the lab.</p> <p>The right hand screen allows you to test questions.</p> <ul> <li>try out some questions in here.</li> </ul> <p>The Semantic model is stored as a yaml file within a stage.  You can save the contents of this model as a semantic view.</p> <ul> <li>exit back to the cortex analyst home screen and click on the 3 dots next to the yaml file to Convert to Semantic View</li> </ul> <p></p> <ul> <li>Convert using the following database and schema</li> </ul> <p></p> <ul> <li>Press Convert and save</li> </ul>"},{"location":"analyst.html#optional-exercise","title":"Optional Exercise","text":"<p>So far, you have browsed an existing Semantic model which is stored as a YAML file. In here, you would be able to edit as you see fit. In addition, you have imported the semantic model as a semantic view which stores the model within the database.</p> <p>It is also easy to create a brand new model from scratch.  If you have time, please have a go at creating a new model.</p> <ul> <li> <p>Press Create New to create a new Semantic model about the previously loaded dataset.</p> </li> <li> <p>Call the model stock_data.</p> </li> </ul> <p>Below you will see the Semantic Model wizard.  This will create a YAML file which makes sense of the data ane provides a link between the sorts of questions that might be asked and the dataset itself.  </p> <ul> <li>Populate the fields as the screenshot below:</li> </ul> <p></p> <ul> <li> <p>Press Next</p> </li> <li> <p>Under the ACCELERATE_AI_IN_FSI.DEFAULT_SCHEMA view, select the STOCK_PRICES table.</p> </li> <li>Press Next: Select tables</li> <li> <p>Select all fields</p> <p></p> </li> <li> <p>Press Done</p> </li> </ul> <p>Next, you will need to specify what fields are Dimensions, Time Dimensions, Facts, Named Fields or Metrics.  Cortex suggests where they should be placed - at this point, you have the ability to override where needed.</p> <p>Under Named filters you can educate Cortex Analyst valid examples of how the data is filtered which improves the accuracy of the answers.  We will add one now</p> <ul> <li>Under Named Filters click on the +</li> <li>Under Expression, type in the following:</li> </ul> <p></p><pre><code>TICKER = 'SNOW'\n</code></pre> - Under Filter name call the filter SNOW. - Press Generate Fields to allow AI to ender a field description with synonyms.  You will see that Cortex assumes we are talking about 'Actual Snow' as apposed to a Data and AI platform.  -   Modify the Synonyms to more relevant ones.  When finished, press Save<p></p> <p>The next step you can provide sample questions of what might be asked about the dataset.  Tryout the following in the text box:</p> <pre><code>What is the latest stock price for SNOW share?\n\nTell me the stock prices for SNOW Shares last week by day of the week?\n\nWhat is the stock prices for SNOW shares by month?\n</code></pre> <p>As you try this out, you will see example outputs</p> <p></p>"},{"location":"analyst.html#add-more-datasets-in-the-model","title":"Add More Datasets in the model","text":"<p>We have now created a Semantic Model for the structured dataset found on the marketplace. If you remember, we parsed two additional datasets from Document AI and created two additional structured tables. We will add these additional tables into the semantic model.</p> <ul> <li>Within the Snowflake AI and ML Studio, click on Cortex Analyst</li> <li>Choose ACCELERATE_AI_IN_FSI.CORTEX_ANALYST for the Database and Schema</li> <li>Choose Cortex Analyst for the STAGE.</li> <li> <p>Select the previously built yaml file.</p> </li> <li> <p>Press Open</p> </li> </ul> <p>Now we are in the edit screen.  You will see the existing setup - but now, we will add an additional table.</p> <ul> <li> <p>Click on the + next to Logical tables.</p> </li> <li> <p>Under ACCELERATE_AI_IN_FSI.DOCUMENT_AI, select EARNINGS_INFOGRAPHIC_PARSED.</p> </li> </ul> <p></p> <ul> <li> <p>Press Next</p> </li> <li> <p>Select all fields.</p> </li> <li> <p>Press Done</p> </li> </ul> <p>You can now browse through the Dimensions, Time Dimensions and Facts to see if everything is as expected.  You have the opportunity to make amendments and add Synonyms to each field</p> <p>You can now test Cortex Analyst out from here.  For instance:</p> <ul> <li> <p>In the prompt ask What is the product Revenue in 2023</p> <p>Cortex will Present you with an answer as well as a sample query.  If you think this is correct, you can add it as a verified Query.</p> <p></p> </li> </ul> <p>When you have finished editing the Semantic Model press Save which is at the top right hand corner of the screen.</p> <ul> <li> <p>Add another Logical Table the same way as before.  This time, within ACCELERATE_AI_IN_FSI.DOCUMENT_AI, select REPORTS_STRUCTURED.</p> </li> <li> <p>Choose All columns and then click Done</p> </li> <li> <p>What analyst gave a rating of Buy?</p> </li> <li> <p>Who gave a rating of Sell?</p> <p></p> </li> <li> <p>When you have finished refining the Semantic Model press Save which is on the top right hand corner of the screen.</p> <p></p> </li> </ul> <p>Add transcripts to the model</p> <p>You will have remembered that we also produced structured data from sound files.  Wouldn't it be good if you could also ask Cortex Analyst when sections of the call are negative or positive??</p> <ul> <li> <p>Add an additional LOGICAL TABLE to the existing semantic model </p> </li> <li> <p>Use the TRANSCRIPTS_BY_MINUTE table which is inside the DEFAULT_SCHEMA</p> </li> <li> <p>Take a look at the facts, Dimensions, synonyms etc of each field - to help Cortex understand the context</p> </li> <li> <p>Try out cortex analyst with the prompt.  See if you can get a good answer for \"what transcript is the most negative?\"</p> </li> <li> <p>Explore other questions users might ask, and refine the model accordingly.</p> </li> <li> <p>Save any changes to the model.  The more descriptive your model, the better the results.  </p> </li> </ul> <p>Remember to Save your work as we will be using this semantic model in the next step.</p> <p>So now we have a way to find out information about Structured Data, and we created a search service for Unstructured Data.</p> <p>Let's create an Application to combine the two types of searches together.  </p> <ul> <li>Please proceed to the Application section where you will learn about Creating an Agent using Snowflake Intelligence</li> </ul>"},{"location":"analyst_2.html","title":"Cortex Analyst - Add More Datasets","text":""},{"location":"analyst_2.html#cortex-analyst-add-more-datasets","title":"Cortex Analyst - Add More Datasets","text":"<p>We have now created a Semantic Model for the structured dataset found on the marketplace. If you remember, we parsed two additional datasets from Document AI and created two additional structured tables. We will add these additional tables into the semantic model.</p> <ul> <li>Within the Snowflake AI and ML Studio, click on Cortex Analyst</li> <li>Choose ACCELERATE_AI_IN_FSI.CORTEX_ANALYST for the Database and Schema</li> <li>Choose Cortex Analyst for the STAGE.</li> <li> <p>Select the previously built yaml file.</p> </li> <li> <p>Press Open</p> </li> </ul> <p>Now we are in the edit screen.  You will see the existing setup - but now, we will add an additional table.</p> <ul> <li> <p>Click on the + next to Logical tables.</p> </li> <li> <p>Under ACCELERATE_AI_IN_FSI.DOCUMENT_AI, select EARNINGS_INFOGRAPHIC_PARSED.</p> </li> </ul> <p></p> <ul> <li> <p>Press Next</p> </li> <li> <p>Select all fields.</p> </li> <li> <p>Press Done</p> </li> </ul> <p>You can now browse through the Dimensions, Time Dimensions and Facts to see if everything is as expected.  You have the opportunity to make amendments and add Synonyms to each field</p> <p>You can now test Cortex Analyst out from here.  For instance:</p> <ul> <li> <p>In the prompt ask What is the product Revenue in 2023</p> <p>Cortex will Present you with an answer as well as a sample query.  If you think this is correct, you can add it as a verified Query.</p> <p></p> </li> </ul> <p>When you have finished editing the Semantic Model press Save which is at the top right hand corner of the screen.</p> <ul> <li> <p>Add another Logical Table the same way as before.  This time, within ACCELERATE_AI_IN_FSI.DOCUMENT_AI, select REPORTS_STRUCTURED.</p> </li> <li> <p>Choose All columns and then click Done</p> </li> <li> <p>What analyst gave a rating of Buy?</p> </li> <li> <p>Who gave a rating of Sell?</p> <p></p> </li> <li> <p>When you have finished refining the Semantic Model press Save which is on the top right hand corner of the screen.</p> <p></p> </li> </ul>"},{"location":"analyst_2.html#optional-exercise","title":"Optional Exercise","text":"<p>You will have remembered that we also produced structured data from sound files.  Wouldn't it be good if you could also ask Cortex Analyst when sections of the call are negative or positive??</p> <ul> <li> <p>Add an additional LOGICAL TABLE to the existing semantic model </p> </li> <li> <p>Use the TRANSCRIPTS_BY_MINUTE table which is inside the DEFAULT_SCHEMA</p> </li> <li> <p>Take a look at the facts, Dimensions, synonyms etc of each field - to help Cortex understand the context</p> </li> <li> <p>Try out cortex analyst with the prompt.  See if you can get a good answer for \"what transcript is the most negative?\"</p> </li> <li> <p>Explore other questions users might ask, and refine the model accordingly.</p> </li> <li> <p>Save any changes to the model.  The more descriptive your model, the better the results.  </p> </li> </ul> <p>Remember to Save your work as we will be using this semantic model in the next step.</p> <p>So now we have a way to find out information about Structured Data, and we created a search service for Unstructured Data.</p> <p>Let's create an Application to combine the two types of searches together.  </p> <ul> <li>Please proceed to the Application section where you will learn about Cortex Agents</li> </ul>"},{"location":"document_ai%20copy.html","title":"Document AI","text":""},{"location":"document_ai%20copy.html#document-ai","title":"Document AI","text":""},{"location":"document_ai%20copy.html#build-and-run-two-document-ai-models","title":"Build and Run Two Document AI models","text":"<p>In this section you will learn how to build and train a Document AI Model which will enable you to extract important structured content out of unstructured documents.</p> <ul> <li> <p>Within the AI &amp; ML area, click on Document AI:</p> <p></p> </li> <li> <p>Click on the + Build icon to build a new model</p> <p></p> </li> <li> <p>Under Build name, type in ANALYST_REPORTS</p> </li> <li> <p>Choose the DATAOPS_EVENT_PROD for the database and DOCUMENT_AI for the Schema</p> </li> </ul> <p></p> <p>Press Create</p>"},{"location":"document_ai%20copy.html#downloading-documents","title":"Downloading Documents","text":"<p>We now need to upload some documents into Document AI in order to build a trained model. The first Model we will build are analyst reports. The reports we will be using are completely fictional.</p> <ul> <li>Download The Analyst Reports here</li> </ul>"},{"location":"document_ai%20copy.html#uploading-the-documents-to-the-model","title":"Document AI","text":"Uploading the Documents to the model <ul> <li>Unzip the Analyst reports to a file location which you can access</li> <li>Open the model which you have just created and press upload documents</li> </ul> <p>You will see that you can upload a variety of formats.  Today we will be uploading PDF documents.  You can either select the files by searching in your file system or drag and drop.</p> <p></p> <p>Press Upload</p> <p>You will see that 6 documents are now uploaded but need attention.  To use the model, you will first need to decide which fields you would like to extract.  A warning will also let you know that for better accuracy you should really train the model with more documents.  For this exercise we will just use 6 but in reality you will need to train the model with at least 10 documents for good accuracy.</p>"},{"location":"document_ai%20copy.html#review-documents-and-extract-fields","title":"Review Documents and Extract Fields","text":"<ul> <li>With in the Documents in Dataset section, press See all documents</li> </ul> <p>You will see that all 6 documents are in need of review</p> <ul> <li> <p>Click on the first document in the list.  This will take you straight to the Analyst Report.  The right hand side of the screen you will need to add the fields that you wish to extract.  We will be extracting specific fields.</p> </li> <li> <p>Press + Entity to add a new field</p> </li> <li> <p>Enter DATE_REPORT in the first field and in the second field, as the question When was the Report Created?</p> </li> </ul> <p>Document AI will search through the multi page document and then will give you an answer with a probability score</p> <p></p> <ul> <li> <p>If you are happy with the answer, mark it with the tick. When you press the tick, the probability score will change to 1.00 - In other words, you have told Document AI that you are 100% sure that this answer is correct.</p> </li> <li> <p>Create 5 additional fields with the corresponding questions - If Document AI gets the answer right, mark with a tick. Press + Entity to create each field.</p> </li> </ul> Field Question NAME_OF_REPORT_PROVIDER name of the report provider RATING What is the rating? is it BUY, SELL, HOLD OR EQUAL-WEIGHT? CLOSE_PRICE Close Price Value PRICE_TARGET Price Target Value GROWTH What is the latest revenue Growth - YoY <p>If the answer is wrong, overtype the answer with the correct answer.  This will automatically mark it as correct with the new answer.</p> <p>Sometimes document AI comes up with no answer at all.  If this is correct, still mark with a tick.</p> <p>When you have finished entering the questions and getting the answers you should see something that looks like this:</p> <p></p> <p>Now it's time to evaluate the same questions and answers with another document.</p> <ul> <li>Click on the arrow to review on the bottom right of this pane the next document.  Currently you are reviewing 1 of 6 documents.</li> </ul> <p>The next document may populate more of the answers.  You will need to simply mark them as to whether they are correct or not.  If any are incorrect, type over with the correct answer.</p> <ul> <li>When you have finished reviewing 2 of 6 documents, continue to review the final 5 documents. </li> </ul> <p>Once you have completed your document review press the back button on the top left to go back to the previous screen</p> <p></p> <p>All 6 documents should now be marked as Accepted. </p> <ul> <li>Go back to Build Details</li> </ul> <p>You will see that there is a model accuracy.  This accuracy figure will be due to how many answers document ai initially predicted correctly or not.  If the accuracy score is low, you may wish to train the model by clicking on Train model button. This will retrain the model using the corrected answers.</p> <ul> <li>Press Publish Version  which will publish your first version of the model.  Today, we will not train model as this can take some time - especially if you are training large documents of multiple pages.  In addition, the model is calculating a high model accuracy score of 0.95.  Being concise with your questions can help the accuracy with little need for training.</li> </ul> <p>Once complete, you will see a model version number and example queries on how to utilise the model in SQL</p> <p></p> <p>You will have examples of these queries preloaded into a notebook. This model is iterative, if you find the results are not what you expect in terms of accuracy, simply add more documents and retrain.  The more documents you train, the better results you will get.  Every time you re-train or republish a new version, you will get a new version number.  You will need this version number in the query.  The screenshot above, shows that you are using the model version 1.</p> <ul> <li>Press the back button to return to the Document AI home screen.</li> </ul>"},{"location":"document_ai%20copy.html#create-a-second-model","title":"Create a second model","text":"<p>The second model we will build are consistent infographic style reports. These are quarterly publicly available Snowflake reports.</p> <ul> <li>Download the Infographics  here</li> </ul> <p>You will now create a new model - this time, we will call it INFOGRAPHICS</p> <ul> <li>Go back to the document AI home page and click on the + build button to build a new model and populate the fields with the following:</li> </ul> <p>Build name: INFOGRAPHICS Location: </p> <ul> <li>Database DATAOPS_EVENT_PROD </li> <li>Schema DOCUMENT_AI</li> </ul> <p>Press Create</p> <p>Upload the previously downloaded Infographic files using the Upload documents button.</p> <ul> <li>Train the model with the following fields</li> </ul> Field Question PROD_REVENUE What is the product revenue (choose Q if it specifies) NET_REVENUE_RETENTION What is the net revenue retention rate TOTAL_CUSTOMERS TOTAL CUSTOMERS TOTAL_M_CUSTOMERS Number of $1 + Customers GLOBAL_2000_CUSTOMERS How many forbes global 2000 customers are there? MARKETPLACE_LISTINGS how many marketplace listings are there? DATE_OF_REPORT What is the date of the report QUARTER What is the quarter of the report Q1, Q2, Q3 or Q4 <ul> <li>Add all the fields the same way as previously</li> <li>Remember to evaluate each field by pressing the tick when correct, or correcting manually if incorrect</li> <li>Remember to review ALL documents</li> </ul> <p>You should finish with answers similar to this.  Remember to correct answers which are incorrect.</p> <p></p> <ul> <li>Press Publish Version to publish a new version.  Do not press train as this may take too much time for the lab.  The accuracy score is also very high as the model has given the correct answers without training.</li> </ul> <p>You will now have a second model which will allow you to extract the infographic information into a structured table.</p> <p>You have now created two models to process documents.  </p>"},{"location":"document_ai%20copy.html#process-documents-at-scale","title":"Process Documents at Scale","text":"<p>Let's now process some documents.  We have more documents both for Analyst Reports and infographics in a stage.  We can use our new models to extract structured information out of them.  In addition, we will extract all the text out of the Analyst reports for search purposes.</p> <p>For the next Document AI steps, you will use Snowflake Notebooks to process the documents as well as visualise the outputs.</p>"},{"location":"document_ai%20copy.html#model-1-analyst-reports","title":"Model 1 - Analyst Reports","text":"<ul> <li>Go back to the home page and click on Projects &gt; Notebooks</li> </ul> <p> - Click on the Document AI Analyst Reports notebook</p> <ul> <li>Run through the notebook to process the Analyst Reports.</li> </ul> <p>This notebook will perform the following:</p> <ul> <li> <p>Extract the values defined in the ANALYST_REPORTS model you have created from a directory of files residing in a Snowflake Stage</p> </li> <li> <p>Create a structured table of information</p> </li> <li> <p>Extract all remaining text using Cortex Parse Document</p> </li> </ul>"},{"location":"document_ai%20copy.html#model-2-infographics","title":"Model 2 - Infographics","text":"<p>You will run a second notebook which will also extract the structured text out of the Infographic image files stored in a Snowflake Stage.</p> <ul> <li>Go back to the Notebooks and open the INFOGRAPHICS notebook</li> </ul> <p></p> <ul> <li>Run through the notebook to process the Infographics which will extract all values and store the results in a new structured table.</li> </ul>"},{"location":"document_ai.html","title":"Document AI","text":""},{"location":"document_ai.html#document-ai","title":"Document AI","text":""},{"location":"document_ai.html#build-and-run-1-document-ai-model","title":"Build and Run 1 Document AI model","text":"<p>In this section you will learn how to build and train a Document AI Model which will enable you to extract important structured content out of unstructured documents.</p> <ul> <li> <p>Within the AI &amp; ML area, click on Document AI:</p> <p></p> </li> <li> <p>Click on the + Build icon to build a new model</p> <p></p> </li> <li> <p>Under Build name, type in ANALYST_REPORTS</p> </li> <li> <p>Choose the ACCELERATE_AI_IN_FSI for the database and DOCUMENT_AI for the Schema</p> </li> </ul> <p></p> <p>Press Create</p>"},{"location":"document_ai.html#downloading-documents","title":"Downloading Documents","text":"<p>We now need to upload some documents into Document AI in order to build a trained model. The first Model we will build are analyst reports. The reports we will be using are completely fictional.</p> <ul> <li>Download The Analyst Reports here</li> </ul>"},{"location":"document_ai.html#uploading-the-documents-to-the-model","title":"Document AI","text":"Uploading the Documents to the model <ul> <li>Unzip the Analyst reports to a file location which you can access</li> <li>Open the model which you have just created and press upload documents</li> </ul> <p>You will see that you can upload a variety of formats.  Today we will be uploading PDF documents.  You can either select the files by searching in your file system or drag and drop.</p> <p></p> <p>Press Upload</p> <p>You will see that 6 documents are now uploaded but need attention.  To use the model, you will first need to decide which fields you would like to extract.  A warning will also let you know that for better accuracy you should really train the model with more documents.  For this exercise we will just use 6 but in reality you will need to train the model with at least 10 documents for good accuracy.</p>"},{"location":"document_ai.html#review-documents-and-extract-fields","title":"Review Documents and Extract Fields","text":"<ul> <li>With in the Documents in Dataset section, press See all documents</li> </ul> <p>You will see that all 6 documents are in need of review</p> <ul> <li>Click on the first document in the list.  This will take you straight to the Analyst Report.  The right hand side of the screen you will need to add the fields that you wish to extract.  We will be extracting specific fields.</li> </ul> <p>To start with, you will be presented with how you would like Document AI to extract the information. Do you want to extract key singular facts (entities) or would you like to extract table information? You choose either table or entity mode at this point. For this exercise you will extract entities. But feel free to experiment and choose table extraction.  </p> <ul> <li> <p>Press + Entity to add a new field</p> </li> <li> <p>Enter DATE_REPORT in the first field and in the second field, as the question When was the Report Created?</p> </li> </ul> <p>Document AI will search through the multi page document and then will give you an answer with a probability score</p> <p></p> <ul> <li> <p>If you are happy with the answer, mark it with the tick. When you press the tick, the probability score will change to 1.00 - In other words, you have told Document AI that you are 100% sure that this answer is correct.</p> </li> <li> <p>Create 5 additional fields with the corresponding questions - If Document AI gets the answer right, mark with a tick. Press + Entity to create each field.</p> </li> </ul> Field Question NAME_OF_REPORT_PROVIDER name of the report provider RATING What is the rating? is it BUY, SELL, HOLD OR EQUAL-WEIGHT? CLOSE_PRICE Close Price Value PRICE_TARGET Price Target Value GROWTH What is the latest revenue Growth - YoY <p>If the answer is wrong, overtype the answer with the correct answer.  This will automatically mark it as correct with the new answer.</p> <p>Sometimes document AI comes up with no answer at all.  If this is correct, still mark with a tick.</p> <p>When you have finished entering the questions and getting the answers you should see something that looks like this:</p> <p></p> <p>Now it's time to evaluate the same questions and answers with another document.</p> <ul> <li>Click on the arrow to review on the bottom right of this pane the next document.  Currently you are reviewing 1 of 6 documents.</li> </ul> <p>The next document may populate more of the answers.  You will need to simply mark them as to whether they are correct or not.  If any are incorrect, type over with the correct answer.</p> <ul> <li>When you have finished reviewing 2 of 6 documents, continue to review the final 5 documents. </li> </ul> <p>Once you have completed your document review press the back button on the top left to go back to the previous screen</p> <p></p> <p>All 6 documents should now be marked as Accepted. </p> <ul> <li>Go back to Build Details</li> </ul> <p>You will see that there is a model accuracy.  This accuracy figure will be due to how many answers document ai initially predicted correctly or not.  If the accuracy score is low, you may wish to train the model by clicking on Train model button. This will retrain the model using the corrected answers.</p> <ul> <li>Press Publish Version  which will publish your first version of the model.  Today, we will not train model as this can take some time - especially if you are training large documents of multiple pages.  In addition, the model is calculating a high model accuracy score of 0.95.  Being concise with your questions can help the accuracy with little need for training.</li> </ul> <p>Once complete, you will see a model version number and example queries on how to utilise the model in SQL</p> <p></p> <p>You will have examples of these queries preloaded into a notebook. This model is iterative, if you find the results are not what you expect in terms of accuracy, simply add more documents and retrain.  The more documents you train, the better results you will get.  Every time you re-train or republish a new version, you will get a new version number.  You will need this version number in the query.  The screenshot above, shows that you are using the model version 1.</p> <ul> <li>Press the back button to return to the Document AI home screen.</li> </ul>"},{"location":"document_ai.html#extracting-data-into-a-table","title":"Extracting data into a table","text":"<p>If you choose the table mode you can extract data into a table format.  This could look something simular to this infograhic example:</p> <p></p> <p>The edit mode allows you to choose what columns to add in the table and multiple tables can be extracted</p> <p></p> <ul> <li>If you wish to tryout the table mode, Download the Infographics  here</li> </ul>"},{"location":"document_ai.html#process-documents-at-scale","title":"Process Documents at Scale","text":"<p>Let's now process some documents.  We have more documents both for Analyst Reports and infographics in a stage.  We can use our new models to extract structured information out of them.  In addition, we will extract all the text out of the Analyst reports for search purposes.</p> <p>For the next Document AI steps, you will use Snowflake Notebooks to process the documents as well as visualise the outputs.</p>"},{"location":"document_ai.html#model-1-analyst-reports","title":"Model 1 - Analyst Reports","text":"<ul> <li>Go back to the home page and click on Projects &gt; Notebooks</li> </ul> <p> - Click on the Document AI Analyst Reports notebook</p> <ul> <li>Run through the notebook to process the Analyst Reports.</li> </ul> <p>This notebook will perform the following:</p> <ul> <li> <p>Extract the values defined in the ANALYST_REPORTS model you have created from a directory of files residing in a Snowflake Stage</p> </li> <li> <p>Create a structured table of information</p> </li> <li> <p>Extract all remaining text using Cortex Parse Document</p> </li> </ul>"},{"location":"document_ai.html#infographics-data","title":"Infographics data","text":"<p>You will run a second notebook which will extract the structured text out of the Infographic image files stored in a Snowflake Stage.</p> <ul> <li>Go back to the Notebooks and open the INFOGRAPHICS notebook</li> </ul> <p></p> <ul> <li>Run through the notebook to process the Infographics which will extract all values and store the results in a new structured table.</li> </ul>"},{"location":"market_place.html","title":"Get Data from the MarketPlace","text":""},{"location":"market_place.html#get-data-from-the-marketplace","title":"Get Data from the MarketPlace","text":"<p>First, it would be good to get some data from the marketplace to understand the latest stock prices of Snowflake shares.</p> <ul> <li>In the Home page, go to Data Products and then Marketplace.</li> <li>In the Search Menu, type in share prices snowflake.</li> <li>Click on the Finance &amp; Economics dataset.</li> </ul> <p></p> <p>This dataset is structured in nature but updates on a daily basis.</p> <ul> <li> <p>Press Get to get the data. You will be prompted to type in your email address if you have not already completed your profile.</p> </li> <li> <p>Press Done</p> </li> </ul> <p>We now have some structured data available to use for analysis</p> <ul> <li>Navigate to Analyse your Data with Cortex Analyst</li> </ul>"},{"location":"search_service.html","title":"Create a Search Service","text":""},{"location":"search_service.html#create-a-search-service","title":"Create a Search Service","text":""},{"location":"search_service.html#introduction","title":"Introduction","text":"<p>In this section, you will create a powerful search service that makes your unstructured financial data instantly accessible and queryable. This is a critical step in building intelligent AI applications that can quickly find and retrieve relevant information from large document collections.</p>"},{"location":"search_service.html#what-youll-accomplish","title":"What You'll Accomplish","text":"<p>By the end of this section, you will have:</p> <ul> <li>Built a Hybrid Search Engine: Created a search service that combines both vector (semantic) and keyword search capabilities</li> <li>Enabled Instant Document Retrieval: Made your analyst reports, earnings call transcripts, and infographics searchable in seconds</li> <li>Implemented RAG Foundations: Established the core infrastructure for Retrieval Augmented Generation (RAG) that powers intelligent AI responses</li> <li>Prepared for Agent Integration: Created the search capabilities that your final AI agent will use to find relevant context</li> </ul>"},{"location":"search_service.html#the-challenge-finding-information-in-unstructured-data","title":"The Challenge: Finding Information in Unstructured Data","text":"<p>You've successfully processed and extracted valuable insights from multiple types of financial documents: - Analyst Reports: Structured data extraction plus full text content - Earnings Call Transcripts: Audio-to-text conversion with sentiment analysis - Financial Infographics: Key metrics extraction from visual content</p> <p>But now you face a common challenge: How do you quickly find specific information within this growing collection of unstructured data?</p>"},{"location":"search_service.html#the-solution-intelligent-search-services","title":"The Solution: Intelligent Search Services","text":"<p>Traditional keyword search falls short when dealing with financial content because: - Important concepts may be expressed in different ways (\"revenue growth\" vs \"top-line expansion\") - Users need to find documents by meaning, not just exact word matches - Context and semantic understanding are crucial for financial analysis</p> <p>Cortex Search solves this by providing: - Semantic Search: Understands meaning and context, not just keywords - Hybrid Approach: Combines the precision of keyword search with the intelligence of vector search - Zero Infrastructure: No need to manage embeddings, indexes, or search infrastructure - Real-time Updates: Automatically refreshes as your data changes</p>"},{"location":"search_service.html#building-blocks-for-ai-intelligence","title":"Building Blocks for AI Intelligence","text":"<p>The search service you create here will become a fundamental component of your final AI agent, enabling it to: - Find Relevant Context: Quickly locate information needed to answer user questions - Provide Citations: Reference specific documents and passages that support its responses - Cross-reference Information: Connect insights from multiple documents and data sources - Support Complex Queries: Handle sophisticated financial analysis requests</p> <p>Let's begin building your intelligent search capability.</p>"},{"location":"search_service.html#understanding-your-data-assets","title":"Understanding Your Data Assets","text":"<p>We now have quite a bit of unstructured data from the analyst reports, the earnings calls, and the infographics - now how do we find it all efficiently? </p> <p>This is where you will build a search service.</p> <p>You have experienced the key principle of RAG based searching while you were searching for relevant pieces of information in the earnings call transcripts section.</p> <p>A search service uses this principle but simplifies the setup.</p> <p>Cortex Search gets you up and running with a hybrid (vector and keyword) search engine on your text data in minutes, without having to worry about embedding, infrastructure maintenance, search quality parameter tuning, or ongoing index refreshes. This means you can spend less time on infrastructure and search quality tuning, and more time developing high-quality chat and search experiences using your data. </p>"},{"location":"search_service.html#implementing-your-search-service","title":"Implementing Your Search Service","text":"<p>Now let's put this into practice by creating your search service using the data you've processed in previous sections.</p> <ul> <li>Go back to the home page and click on Projects &gt; Notebooks</li> </ul> <p></p> <ul> <li>Open the CREATE_SEARCH_SERVICE notebook</li> </ul>"},{"location":"search_service.html#next-steps-connecting-to-structured-data","title":"Next Steps: Connecting to Structured Data","text":"<p>Excellent! You have now created a powerful search service for your unstructured financial documents. Your search service can now: - Find relevant analyst reports based on semantic meaning - Retrieve earnings call segments related to specific topics - Search through infographic content and extracted insights</p> <p>However, comprehensive financial analysis requires both unstructured insights and structured data analysis. While you've used Cortex Search to find information within unstructured documents, there are also many structured datasets that you'll want to include in your analysis. Some structured tables you have already created using Document AI, but there will be additional structured datasets from external sources.</p> <ul> <li> <p>Proceed to Structured Data Processing where you will start by transforming stock prices and trading datasets from the marketplace.</p> <p>Click Here to continue</p> </li> </ul>"},{"location":"snowflake_intelligence.html","title":"Configure and use Snowflake Intelligence","text":""},{"location":"snowflake_intelligence.html#configure-and-use-snowflake-intelligence","title":"Configure and use  Snowflake Intelligence","text":""},{"location":"snowflake_intelligence.html#introduction","title":"Introduction","text":"<p>In this section, you will bring together all the AI capabilities you've built throughout this lab to create a unified, intelligent financial assistant using Snowflake Intelligence. This represents the culmination of your hands-on learning experience, where you'll orchestrate multiple AI tools into a single conversational agent.</p>"},{"location":"snowflake_intelligence.html#what-youll-accomplish","title":"What You'll Accomplish","text":"<p>By the end of this section, you will have:</p> <ul> <li>Created a Financial AI Agent: Built a conversational agent that acts as a knowledgeable financial analyst named \"StockOne\"</li> <li>Integrated Multiple Data Sources: Connected your agent to both structured data (via Cortex Analyst) and unstructured data (via Cortex Search)</li> <li>Unified Analysis Capabilities: Enabled seamless querying across stock prices, analyst reports, earnings calls, and financial infographics</li> <li>Demonstrated Real-world Applications: Shown how AI can synthesize complex financial information to support investment decisions</li> </ul>"},{"location":"snowflake_intelligence.html#building-on-your-previous-work","title":"Building on Your Previous Work","text":"<p>This section leverages everything you've created so far:</p> <ul> <li>Document AI Models: Your trained models for analyst reports and infographics will provide structured insights</li> <li>Cortex Search Services: Your search services will enable rapid retrieval of relevant information from earnings calls and reports  </li> <li>Cortex Analyst Semantic Models: Your semantic models will allow natural language querying of financial data</li> <li>Marketplace Data: Your connected stock price data will provide real-time market information</li> </ul>"},{"location":"snowflake_intelligence.html#what-is-snowflake-intelligence","title":"What is Snowflake Intelligence?","text":"<p>Snowflake Intelligence is a comprehensive AI platform that allows you to create sophisticated agents by combining multiple AI tools and data sources. Unlike simple chatbots, these agents can:</p> <ul> <li>Reason Across Data Types: Combine insights from structured databases and unstructured documents</li> <li>Use Specialized Tools: Leverage different AI capabilities (search, analysis, visualization) as needed</li> <li>Provide Contextual Responses: Give detailed, evidence-backed answers with citations and visualizations</li> <li>Learn from Multiple Sources: Draw insights from your complete data ecosystem</li> </ul> <p>Let's begin by creating your financial intelligence agent that will demonstrate the power of unified AI-driven analysis.</p>"},{"location":"snowflake_intelligence.html#creating-your-financial-agent","title":"Creating Your Financial Agent","text":"<p>You can easily create your own agent using Snowflake Intelligence</p> <ul> <li>Within AI &amp; ML  click on Agents</li> <li> <p>Press Create Agent</p> </li> <li> <p>Fill in the text boxes provided.</p> </li> </ul> <p></p> <ul> <li>Press Create agent</li> </ul> <p></p> <ul> <li>Click on the agent and then press Edit</li> <li>Populate the about description - similar to the example below:</li> </ul> <p></p> <ul> <li> <p>Under Instructions, fill in the response instructions as follows:</p> <p>Response Instruction</p> <p>You are a financial analyst called StockOne.  You are using the latest information about Snowflake to give the user expert advice.</p> <p>Sample Questions</p> <p>give me a table of ratings for each analyst?</p> <p>what happened in the last earnings call?</p> <p>Is Snowflake expected to grow in the future?</p> <p>what are the latest SNOW stock prices over time in the last 12 months?</p> <p>how many marketplace listings are in the latest report?</p> <p>What did Shridhar say about revenue in the last earnings call?</p> <p>shall I buy Snowflake shares?</p> <p>Give me a chart which analyses sentiment on the earnings calls</p> </li> </ul>"},{"location":"snowflake_intelligence.html#adding-tools-to-the-agent","title":"Adding Tools to the Agent","text":"<p>Snowflake Intelligence allows for multiple tools to be added to allow the agent to answer the question. The standard snowflake tools you can add are search services and Cortex Analyst.</p> <p>In addition, you can add custom tools - such as a tool to generate an email or using other external services.</p> <ul> <li>Under Tools click +add next to Cortex Analyst to add the defined semantic model</li> </ul> <p>You can select the model either as a view or via the Semantic Model File.</p> <p></p> <p>Choose DEFAULT_WH for the warehouse. This is the warehouse used to process the queries.</p> <p>Under query Timeout, add the max number of seconds that the agent is allowed to think before timing out. In this example, enter 20.</p> <ul> <li> <p>Finally enter a description for the tool. Tip Generate with Cortex to automatically provide a meaningful description. Allow time for the description to be generated.</p> </li> <li> <p>Press Add to add the tool.</p> </li> </ul> <p></p> <p>Next you will create tools for the unstructured data.  These will be linked to Search Services</p> <ul> <li> <p>Under Cortex Search Services, select +Add to create to assign a new search service</p> </li> <li> <p>Complete the configuration as per the screenshot below.</p> </li> </ul> <p></p> <p>You will be assigning the URL in order to drill down to documents when appropriate.</p> <ul> <li> <p>Press Add</p> </li> <li> <p>Create a Second Search Service Tool - this time, for the Infographics</p> </li> </ul> <p></p> <p>Orchestration Next you will need to choose the LLM in order to orchestrate the questions provided by the consumer. This will be in the Orchestration section. You will see which LLMs are available. If you wish, let the agent decide by leaving auto set.</p> <p>You can also prompt the agent further by how to break down the tasks</p> <p>Press Save to save the agent.</p> <p>Snowflake Intelligence</p> <p>Now you have finished the agent setup, you can now go to Snowflake Intelligence and ask questions.</p> <ul> <li> <p>From AI &amp; ML, navigate to Snowflake intelligence</p> </li> <li> <p>Ask a question</p> </li> </ul> <p>Example question</p> <p>*Give me a table of ratings for each analyst?</p> <p>what happened in the last earnings call?</p> <p>What did Morgan Stanley say about Growth?</p> <p>what are the latest SNOW stock prices over time in the last 12 months?</p> <p>how many marketplace listings are in the latest report?</p> <p>Tell me about dynamic tables?</p> <p>What did Shridhar say about revenue in the last earnings call?</p> <p>shall I buy Snowflake shares?*</p> <p></p> <p>You will note that this question will look at a variety of data sources which provides the answer.</p>"},{"location":"snowflake_intelligence.html#conclusion","title":"Conclusion","text":"<p>Congratulations! You have successfully built a comprehensive AI-powered financial analysis assistant using Snowflake Intelligence. Throughout this hands-on lab, you have accomplished several key objectives:</p>"},{"location":"snowflake_intelligence.html#data-integration-and-processing","title":"Data Integration and Processing","text":"<p>You have successfully integrated and processed multiple types of financial data:</p> <ul> <li>Real-time Stock Data: Connected to marketplace data to access current and historical Snowflake (SNOW) stock prices</li> <li>Analyst Reports: Used Document AI to extract structured insights from fictional analyst reports, including ratings (BUY/SELL/HOLD), price targets, and growth projections</li> <li>Financial Infographics: Processed quarterly earnings infographics to extract key metrics like revenue, customer counts, and marketplace listings</li> <li>Earnings Call Transcripts: Analyzed audio recordings from quarterly earnings calls, including sentiment analysis and key executive statements</li> </ul>"},{"location":"snowflake_intelligence.html#ai-technologies-mastered","title":"AI Technologies Mastered","text":"<p>You have hands-on experience with cutting-edge AI capabilities:</p> <ul> <li>Document AI: Built and trained custom models to extract structured data from unstructured financial documents</li> <li>Cortex Search: Created search services for rapid retrieval of relevant information from large document collections</li> <li>Cortex Analyst: Developed semantic models to enable natural language querying of structured financial data</li> <li>Snowflake Intelligence: Orchestrated multiple AI tools into a unified conversational agent</li> </ul>"},{"location":"snowflake_intelligence.html#unified-financial-decision-support","title":"Unified Financial Decision Support","text":"<p>Your agent can now provide comprehensive investment insights by:</p> <ul> <li>Combining Multiple Perspectives: Integrating quantitative stock data with qualitative analyst opinions and executive commentary</li> <li>Time-based Analysis: Analyzing trends over time from historical stock prices, quarterly reports, and evolving analyst ratings</li> <li>Sentiment Understanding: Assessing market sentiment through earnings call analysis and analyst report tone</li> <li>Real-time Intelligence: Providing up-to-date information synthesis to support investment decisions</li> </ul>"},{"location":"snowflake_intelligence.html#real-world-applications","title":"Real-world Applications","text":"<p>The AI assistant you've built demonstrates practical applications for:</p> <ul> <li>Investment Research: Quickly synthesizing information from multiple sources to evaluate investment opportunities</li> <li>Risk Assessment: Understanding both quantitative metrics and qualitative factors that could impact stock performance</li> <li>Market Analysis: Combining structured financial data with unstructured market commentary for comprehensive analysis</li> <li>Decision Support: Providing evidence-based recommendations backed by multiple data sources</li> </ul>"},{"location":"snowflake_intelligence.html#key-takeaway","title":"Key Takeaway","text":"<p>You have experienced firsthand how Snowflake's AI Data Cloud can transform raw, disparate financial information into actionable intelligence. By combining structured data (stock prices, financial metrics) with unstructured data (analyst reports, earnings calls), your AI agent can now answer complex questions like \"Should I buy Snowflake shares?\" with comprehensive, data-driven insights.</p> <p>This approach represents the future of financial analysis - where AI enables rapid synthesis of vast amounts of information to support better investment decisions. The techniques you've learned can be applied to any stock, sector, or financial decision-making scenario.</p> <p>Remember: While this lab demonstrates powerful AI capabilities for financial analysis, all analyst reports used were fictitious, and no actual investment decisions should be made based on this lab's outputs.</p>"},{"location":"sound_transcripts.html","title":"Analyst Investor Calls","text":""},{"location":"sound_transcripts.html#analyst-investor-calls","title":"Analyst Investor Calls","text":"<p>Snowflake Inc., like all publicly listed companies, conducts public announcements of its quarterly financial results. In this project, we will process the Q3 earnings call recording featuring Snowflake\u2019s CEO and CFO. The workflow involves converting the MP3 audio file into text using transcription technology. Once transcribed, the text will be summarized to extract key insights. Finally, this summary will be stored in a dedicated Summary table for further analysis. We will add this unstructured data summary into our Cortex Agent at the end of the lab. This will show you the power of converted calls into text and then into Summary using cortex and then front ending them using Streamlit so you have an Agentic AI.</p> <p></p> <p>Let's Begin analysing sound transcript data - Go back to the home page and click on Projects &gt; Notebooks</p> <p></p> <ul> <li>Click on the ANALYSE_SOUND notebook</li> <li>Run through the notebook to analyse the sound transcripts which also includes sentiment analysis and an introduction to how Vector Embeddings enhances search capabilities.</li> </ul>"},{"location":"assets/docai/step1.html","title":"1 - Logging in and ready to start","text":""},{"location":"assets/docai/step1.html#1-logging-in-and-ready-to-start","title":"1 - Logging in and ready to start","text":"<p>If you are here, you have already completed the registration process, and your prebuilt Snowflake environment details are shown on your event page. It will look similar to:</p> <p></p> <p>You can always get back to your registration page and see your information by clicking here: Personalized Event Homepage</p>"}]}